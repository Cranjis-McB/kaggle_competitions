{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vikramsandu/dl-nlp-commonlit-summaries?scriptVersionId=141167394\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Goal\n\n* The immediate goal is to setup this notebook for training, Inference and submission for LB evaluation. <font color=\"green\">&#10004;</font>\n* Efficient Back Translation of the Data for Augmentations. <font color=\"green\">&#10004;</font> [Notebook Link](https://www.kaggle.com/code/vikramsandu/dl-nlp-commonlit-summaries)\n* First Finetune the MLM Model on the CommonLit Dataset and then use it for the downstream task. <font color=\"green\">&#10004;</font>  [Notebook Link](https://www.kaggle.com/code/vikramsandu/commonlit-mlm)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Let's Create a Controller variable for Training/Submission Mode.\n\n* Set **True** for Submission Mode.\n* Set **False** for Training Mode.","metadata":{}},{"cell_type":"code","source":"# True if you want to submit the Notebook\nisSubmit = False\n\nif isSubmit:\n    print(f'This notebook is on submission Mode...')\n    print(f'Make sure to turn off the Internet...')\nelse:\n    print(f'This notebook is on Training Mode...')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:15:55.578217Z","iopub.execute_input":"2023-08-27T12:15:55.578897Z","iopub.status.idle":"2023-08-27T12:15:55.584615Z","shell.execute_reply.started":"2023-08-27T12:15:55.578865Z","shell.execute_reply":"2023-08-27T12:15:55.583686Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"This notebook is on Training Mode...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Read and Understand Data.","metadata":{}},{"cell_type":"markdown","source":"Here we have two type of files: \n1. **<span style=\"color:red\">Prompts:</span>** It contains the Question, a title about the text, and the text that needs to be summarized.\n2. **<span style=\"color:red\">Summaries:</span>** This includes the text summarized by the students, along with the corresponding prompt id and target scores for both content and wording.","metadata":{}},{"cell_type":"code","source":"# Read Dataset\nimport pandas as pd\n\nif not isSubmit:\n    # Train\n    prompt_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n    summary_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\n\n    print(f'\\nLength of Train Prompt df: {len(prompt_df)}')\n    print(f'Length of Train Summary df: {len(summary_df)}\\n')\n\n    # Display\n    display(summary_df.sample(5))","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:15:57.074186Z","iopub.execute_input":"2023-08-27T12:15:57.074563Z","iopub.status.idle":"2023-08-27T12:15:57.200455Z","shell.execute_reply.started":"2023-08-27T12:15:57.074535Z","shell.execute_reply":"2023-08-27T12:15:57.199442Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\nLength of Train Prompt df: 4\nLength of Train Summary df: 7165\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        student_id prompt_id  \\\n498   12e87dcee601    39c16e   \n5271  baeaad549844    814d6b   \n6335  e29cfbfad6fb    3b9047   \n6218  de1a384c805d    ebad26   \n4322  99109e5ee537    3b9047   \n\n                                                   text   content   wording  \n498   Three Elements that are ideal for the perfect ...  0.205683  0.380538  \n5271  Unable to explain to students how people follo...  0.233385 -0.838378  \n6335  The ancient of the goverment remoted to many d... -1.408180 -0.493603  \n6218  They would use chemicals to get rid of the odo...  0.014081 -0.124906  \n4322      The social structure of ancient Egypt had ...  2.375562  1.876502  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>498</th>\n      <td>12e87dcee601</td>\n      <td>39c16e</td>\n      <td>Three Elements that are ideal for the perfect ...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n    </tr>\n    <tr>\n      <th>5271</th>\n      <td>baeaad549844</td>\n      <td>814d6b</td>\n      <td>Unable to explain to students how people follo...</td>\n      <td>0.233385</td>\n      <td>-0.838378</td>\n    </tr>\n    <tr>\n      <th>6335</th>\n      <td>e29cfbfad6fb</td>\n      <td>3b9047</td>\n      <td>The ancient of the goverment remoted to many d...</td>\n      <td>-1.408180</td>\n      <td>-0.493603</td>\n    </tr>\n    <tr>\n      <th>6218</th>\n      <td>de1a384c805d</td>\n      <td>ebad26</td>\n      <td>They would use chemicals to get rid of the odo...</td>\n      <td>0.014081</td>\n      <td>-0.124906</td>\n    </tr>\n    <tr>\n      <th>4322</th>\n      <td>99109e5ee537</td>\n      <td>3b9047</td>\n      <td>The social structure of ancient Egypt had ...</td>\n      <td>2.375562</td>\n      <td>1.876502</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"if not isSubmit:\n    # Distribution of the Prompt Questions.\n    print(summary_df['prompt_id'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:15:57.694108Z","iopub.execute_input":"2023-08-27T12:15:57.695001Z","iopub.status.idle":"2023-08-27T12:15:57.707333Z","shell.execute_reply.started":"2023-08-27T12:15:57.694969Z","shell.execute_reply":"2023-08-27T12:15:57.706089Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"39c16e    2057\n3b9047    2009\nebad26    1996\n814d6b    1103\nName: prompt_id, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"if not isSubmit:\n    # Number of Unique Students\n    print(len(summary_df['student_id'].unique()))","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:15:58.090207Z","iopub.execute_input":"2023-08-27T12:15:58.093417Z","iopub.status.idle":"2023-08-27T12:15:58.103847Z","shell.execute_reply.started":"2023-08-27T12:15:58.093385Z","shell.execute_reply":"2023-08-27T12:15:58.102531Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"7165\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**So the Training data contains only 4 Prompt Questions that is being asked to 7165 students.**","metadata":{}},{"cell_type":"markdown","source":"Now Let's understand the Labels.","metadata":{}},{"cell_type":"code","source":"if not isSubmit: \n    # Describe\n    describe_content_df = summary_df['content'].describe()\n    describe_wording_df = summary_df['wording'].describe()\n\n    print(pd.concat([describe_content_df, describe_wording_df], axis=1))","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:15:59.383962Z","iopub.execute_input":"2023-08-27T12:15:59.384335Z","iopub.status.idle":"2023-08-27T12:15:59.406976Z","shell.execute_reply.started":"2023-08-27T12:15:59.384307Z","shell.execute_reply":"2023-08-27T12:15:59.405692Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"           content      wording\ncount  7165.000000  7165.000000\nmean     -0.014853    -0.063072\nstd       1.043569     1.036048\nmin      -1.729859    -1.962614\n25%      -0.799545    -0.872720\n50%      -0.093814    -0.081769\n75%       0.499660     0.503833\nmax       3.900326     4.310693\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* The value of **Content** ranging from -1.72 to 3.90.\n* Value of the **Wording** is ranging from -1.96 to 4.31.\n\nIt surely has two classes (Content and Wording) and each class has a continuous output. This problem can be put into the category of **Two-Class Regression.**","metadata":{}},{"cell_type":"markdown","source":"Let's have an overall idea about the number of words (or tokens) in summary text.","metadata":{}},{"cell_type":"code","source":"if not isSubmit:\n    text_length = summary_df['text'].apply(lambda x: len(x.split(' ')))\n    print(text_length.describe())\n    print('\\n')\n\n    # Lets Visualize the same.\n    from matplotlib import pyplot as plt\n\n    # Create a histogram using Matplotlib\n    plt.figure(figsize = (3, 3))\n    plt.hist(text_length, bins=10, edgecolor='k')\n    plt.xlabel('Text Length')\n    plt.ylabel('Frequency')\n    plt.title(f'Distribution of the Text Length')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:16:00.625463Z","iopub.execute_input":"2023-08-27T12:16:00.625816Z","iopub.status.idle":"2023-08-27T12:16:00.976116Z","shell.execute_reply.started":"2023-08-27T12:16:00.625788Z","shell.execute_reply":"2023-08-27T12:16:00.975157Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"count    7165.000000\nmean       76.155618\nstd        54.538587\nmin        22.000000\n25%        40.000000\n50%        59.000000\n75%        93.000000\nmax       651.000000\nName: text, dtype: float64\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 300x300 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"The average text summary length is approximately 76 tokens, with a standard deviation of 54 tokens. Most of the text summaries have a length of less than 200 tokens. That is still in the limit of **BERT** and related models, which accept a **maximum of 512 tokens**. In Future, we might use some of the contextual information from the Prompt text to utilize this token gap properly.","metadata":{}},{"cell_type":"markdown","source":"Let's have an overall idea about the number of words (or tokens) in Prompt Text.","metadata":{}},{"cell_type":"code","source":"if not isSubmit:\n    text_length = prompt_df['prompt_text'].apply(lambda x: len(x.split(' ')))\n    # Lets Visualize the same.\n    print(text_length)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:16:02.090119Z","iopub.execute_input":"2023-08-27T12:16:02.090509Z","iopub.status.idle":"2023-08-27T12:16:02.099989Z","shell.execute_reply.started":"2023-08-27T12:16:02.090479Z","shell.execute_reply":"2023-08-27T12:16:02.098741Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0    604\n1    550\n2    597\n3    966\nName: prompt_text, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Prompt Text is ranging from 550 to 966 words.","metadata":{}},{"cell_type":"markdown","source":"**We must provide this contextual information (prompt text) along with the text summary to yield better results.**\n\nBut this will result in a longer document (way longer than transformer can process --512 tokens--), to tackle this we will use longformer.\n\n* Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. \n\nwe'll get back to this later if needed.","metadata":{}},{"cell_type":"markdown","source":"## Dataset and Dataloaders","metadata":{}},{"cell_type":"markdown","source":"Now Let's define the Dataset Class.\n\nThe Dataset class in Natural Language Processing (NLP) serves as a fundamental data structure that helps manage and handle textual data for training and evaluation purposes.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\nclass CommonLitSummaryDataset(Dataset):\n    def __init__(self, \n                 summary_df,\n                 prompt_df, \n                 tokz='/kaggle/input/commonlit-summaries-all-tokenizers/bert_base_cased_tokenizer', \n                 max_length = 256,\n                 isTest = False,\n                ):\n        self.summary_df = summary_df\n        self.prompt_df = prompt_df\n        self.max_length = max_length\n        self.tokz = AutoTokenizer.from_pretrained(tokz)\n        self.isTest = isTest\n        \n    def __len__(self):\n        return len(self.summary_df)\n    \n    def __getitem__(self, idx):\n        \n        # Get the Summary and It's Corresponding Question\n        txt_summary = self.summary_df['text'].iloc[idx]\n        prompt_id = self.summary_df['prompt_id'].iloc[idx]\n        txt_question = self.prompt_df[self.prompt_df['prompt_id'] == prompt_id]['prompt_question'].iloc[0]\n        title = self.prompt_df[self.prompt_df['prompt_id'] == prompt_id]['prompt_title'].iloc[0]\n        \n        # Concat the Question and Summary.\n        # 'TITLE: ' + title + \\\n        input_text = ' QUESTION: ' + txt_question + ' SUMMARY: ' + txt_summary\n        \n        # Convert the text data into Corresponding Numerical Embeddings.\n        encodings = self.tokz.encode_plus(input_text, \n                                          add_special_tokens=True, \n                                          max_length = self.max_length, \n                                          padding = 'max_length', \n                                          truncation = True, \n                                          return_tensors = 'pt'\n                                         )\n        input_ids = encodings['input_ids'].squeeze()\n        attention_mask = encodings['attention_mask'].squeeze()\n        \n        # For Test set, No labels will be available\n        if self.isTest:\n            return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n        }\n            \n        # Labels\n        label = torch.tensor(self.summary_df.iloc[idx][-2:].tolist())\n        \n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': label\n        }","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:53:54.244439Z","iopub.execute_input":"2023-08-27T13:53:54.244801Z","iopub.status.idle":"2023-08-27T13:53:54.258139Z","shell.execute_reply.started":"2023-08-27T13:53:54.244772Z","shell.execute_reply":"2023-08-27T13:53:54.256878Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"if not isSubmit:\n    \n    isStratified = True\n    \n    # Split the dataframe into train and validation sets\n    from sklearn.model_selection import train_test_split\n    \n    if not isStratified:\n        # Normal Split\n        train_df, valid_df = train_test_split(summary_df, test_size=0.2, random_state=42)\n    else:\n        # Stratified Split\n        test_prompt_id = ['3b9047']\n        \n        # Filter the DataFrame to create train and test sets\n        train_df = summary_df[~summary_df['prompt_id'].isin(test_prompt_id)].reset_index(drop=True)\n        valid_df = summary_df[summary_df['prompt_id'].isin(test_prompt_id)].reset_index(drop=True)\n\n    \n    print('Train Prompt IDs: ', train_df['prompt_id'].unique())\n    print('Valid Prompt IDs: ', valid_df['prompt_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:53:54.786794Z","iopub.execute_input":"2023-08-27T13:53:54.78715Z","iopub.status.idle":"2023-08-27T13:53:54.801721Z","shell.execute_reply.started":"2023-08-27T13:53:54.787121Z","shell.execute_reply":"2023-08-27T13:53:54.799822Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Train Prompt IDs:  ['814d6b' 'ebad26' '39c16e']\nValid Prompt IDs:  ['3b9047']\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LENGTH = 256\nTOKZ = \"/kaggle/input/commonlit-summaries-all-tokenizers/deberta_base_v3_tokz/deberta_base_v3_tokz\"\n\nif not isSubmit:\n\n    # Initialize Dataset Classes\n    commonlit_summary_train_ds = CommonLitSummaryDataset(train_df,\n                                                         prompt_df, \n                                                         tokz = TOKZ, \n                                                         max_length = MAX_LENGTH\n                                                        )\n    commonlit_summary_valid_ds = CommonLitSummaryDataset(valid_df, \n                                                         prompt_df,  \n                                                         tokz = TOKZ,  \n                                                         max_length = MAX_LENGTH\n                                                        )\n    print(f'Train - {len(commonlit_summary_train_ds)}, Test - {len(commonlit_summary_valid_ds)}')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:53:55.18325Z","iopub.execute_input":"2023-08-27T13:53:55.183953Z","iopub.status.idle":"2023-08-27T13:53:55.612934Z","shell.execute_reply.started":"2023-08-27T13:53:55.183917Z","shell.execute_reply":"2023-08-27T13:53:55.611938Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Train - 5156, Test - 2009\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's Visualize one Sample to see the working of our Dataset.","metadata":{}},{"cell_type":"code","source":"if not isSubmit:\n\n    # Tokenizer\n    tokz =  AutoTokenizer.from_pretrained(TOKZ)\n\n    print(f'------ Input ----------\\n')\n    sample = commonlit_summary_train_ds[0]\n    print(tokz.decode(sample['input_ids']))\n\n    print(f'\\n------ Labels ----------\\n')\n    labels = sample['labels']\n    print(labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:53:56.756179Z","iopub.execute_input":"2023-08-27T13:53:56.756552Z","iopub.status.idle":"2023-08-27T13:53:57.054537Z","shell.execute_reply.started":"2023-08-27T13:53:56.756521Z","shell.execute_reply":"2023-08-27T13:53:57.053539Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"------ Input ----------\n\n[CLS] QUESTION: Summarize how the Third Wave developed over such a short period of time and why the experiment was ended. SUMMARY: The third wave was an experimentto see how people reacted to a new one leader government. It gained popularity as people wanted to try new things. The students follow anything that is said and start turning on eachother to gain higher power. They had to stop the experement as too many people got to radical with it blindly following there leader[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n\n------ Labels ----------\n\ntensor([0.2057, 0.3805], dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Dataloader:**\n\nDataloader allows us to group multiple samples into batches, enabling parallel processing and more efficient GPU utilization during training.","metadata":{}},{"cell_type":"code","source":"# Dataloader\nfrom torch.utils.data import DataLoader\n\nif not isSubmit:\n    # Create a data loader for the dataset\n    batch_size = 16\n    train_dataloader = DataLoader(commonlit_summary_train_ds, batch_size=batch_size, shuffle=True)\n    eval_dataloader = DataLoader(commonlit_summary_valid_ds, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:53:58.396088Z","iopub.execute_input":"2023-08-27T13:53:58.39676Z","iopub.status.idle":"2023-08-27T13:53:58.569167Z","shell.execute_reply.started":"2023-08-27T13:53:58.396726Z","shell.execute_reply":"2023-08-27T13:53:58.56816Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AdamW\n\nmodel_nm = 'microsoft/deberta-v3-base'\nnum_labels = 2\n\nif not isSubmit:\n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=num_labels)\n    \n    # Freeze Deberta Layers\n    for param in model.deberta.parameters():\n        param.requires_grad = False\n\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"\\nTotal Trainable parameters: \", total_params)\n\n    total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    print(\"Total size (bytes) of the model: \", total_size)\n    print(\"Total size (MB) of the model: \", total_size / (1024 * 1024))","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:54:00.118305Z","iopub.execute_input":"2023-08-27T13:54:00.118691Z","iopub.status.idle":"2023-08-27T13:54:01.96185Z","shell.execute_reply.started":"2023-08-27T13:54:00.11866Z","shell.execute_reply":"2023-08-27T13:54:01.960861Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nTotal Trainable parameters:  592130\nTotal size (bytes) of the model:  737694728\nTotal size (MB) of the model:  703.5205154418945\n","output_type":"stream"}]},{"cell_type":"markdown","source":"we need to save the best model during Training based on the **MCRMSE** (mean columnwise root mean squared error) which is the Cost Function/ Metric for this Competition.\n\nLet's implement this.","metadata":{}},{"cell_type":"code","source":"# Utility: mean columnwise root mean squared error\ndef mcrmse_loss(predictions, targets):\n    rmse_columnwise = torch.sqrt(torch.mean((predictions - targets)**2, dim=0))\n    return torch.mean(rmse_columnwise)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:54:05.454305Z","iopub.execute_input":"2023-08-27T13:54:05.45468Z","iopub.status.idle":"2023-08-27T13:54:05.460157Z","shell.execute_reply.started":"2023-08-27T13:54:05.454649Z","shell.execute_reply":"2023-08-27T13:54:05.458845Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"markdown","source":"In order to have **faster Training**, we will do the followings:\n\n1. **Enable mixed precision training** (using half-precision floating-point format or fp16). It utilizes Tensor Cores on supported NVIDIA GPUs to speed up the training process with reduced memory usage.\n\n2.  To utilize Kaggle T4/X2 GPU, we will use **DataParallel**. DataParallel allows you to train your model on multiple GPUs simultaneously, distributing the workload across all available GPUs.","metadata":{}},{"cell_type":"code","source":"# Import Dataparallel and mixed precision modules\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.parallel import DataParallel\n    \nif not isSubmit:\n\n    # Check if GPU is available.\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'DEVICE: {device}\\n')\n\n    # Used for Mixed Precision Training\n    scaler = GradScaler()\n\n    # Move your model to the GPU and wrap it with DataParallel (To utilize T4X2)\n    model = model.to(device)\n    model = DataParallel(model) ","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:54:10.087419Z","iopub.execute_input":"2023-08-27T13:54:10.087776Z","iopub.status.idle":"2023-08-27T13:54:10.298069Z","shell.execute_reply.started":"2023-08-27T13:54:10.087748Z","shell.execute_reply":"2023-08-27T13:54:10.297101Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"DEVICE: cuda\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's Train the Model.","metadata":{}},{"cell_type":"code","source":"# Visualize progress bar\nfrom tqdm import tqdm\nfrom transformers import AdamW\n\nif not isSubmit:\n    # Prepare optimizer\n    optimizer = AdamW(model.parameters(), lr=1e-5)\n    NUM_EPOCHS = 15\n\n    best_eval_loss = float('inf')  # Initialize the best evaluation loss to infinity\n\n    for epoch in range(NUM_EPOCHS):\n        total_loss = 0\n        model.train() # Set the model to Training mode\n\n        for batch in tqdm(train_dataloader):\n\n            with autocast():\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                # Forward loop\n                optimizer.zero_grad() # Ensures Gradient doesn't accumulate.\n                predictions = model(input_ids=input_ids,\n                                    attention_mask=attention_mask, \n                                    labels=labels\n                                   ).logits\n\n                # Compute MCRMSE loss\n                loss = mcrmse_loss(predictions, labels)\n\n            # BackProp\n            scaler.scale(loss).backward()  # Scale the loss value\n            scaler.step(optimizer)\n            scaler.update()\n\n            # Accumulate the Loss\n            total_loss += loss.item()\n\n        # Calculate epoch-level metrics\n        epoch_loss = total_loss / len(train_dataloader)\n\n        # Print epoch-level metrics\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        print(f\"Train Loss: {epoch_loss:.4f}\")\n\n         # Evaluation\n        model.eval()  # Set model to evaluation mode\n        eval_loss = 0\n\n        with torch.no_grad():\n            for batch in eval_dataloader:\n                with autocast():\n                    input_ids = batch['input_ids'].to(device)\n                    attention_mask = batch['attention_mask'].to(device)\n                    labels = batch['labels'].to(device)\n\n                    predictions = model(input_ids=input_ids,\n                                        attention_mask=attention_mask, \n                                        labels=labels\n                                       ).logits\n                    loss = mcrmse_loss(predictions, labels)\n\n                    eval_loss += loss.item()\n\n        # Calculate evaluation metrics\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n\n        # Print evaluation metrics\n        print(f\"Eval Loss: {eval_epoch_loss:.4f}\")\n\n        # Save the Best Model\n        if eval_epoch_loss < best_eval_loss:\n            print(f'--------------------------------------')\n            print(f'Found the best model at Epoch {epoch+1}')\n            print(f'Validation Loss reduced from {best_eval_loss:.4f} to {eval_epoch_loss:.4f}')\n            best_eval_loss = eval_epoch_loss\n            print(f'Saving the best model.')\n            print(f'--------------------------------------\\n')\n            \n            # Access the actual model from the DataParallel object\n            actual_model = model.module\n            # Save\n            actual_model.save_pretrained(\"debertaBaseV3_3b9047\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T12:26:53.558253Z","iopub.execute_input":"2023-08-27T12:26:53.558616Z","iopub.status.idle":"2023-08-27T12:56:07.790328Z","shell.execute_reply.started":"2023-08-27T12:26:53.558588Z","shell.execute_reply":"2023-08-27T12:56:07.789379Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\nTrain Loss: 0.8622\nEval Loss: 0.7415\n--------------------------------------\nFound the best model at Epoch 1\nValidation Loss reduced from inf to 0.7415\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:35<00:00,  3.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15\nTrain Loss: 0.7657\nEval Loss: 0.6959\n--------------------------------------\nFound the best model at Epoch 2\nValidation Loss reduced from 0.7415 to 0.6959\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:35<00:00,  3.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15\nTrain Loss: 0.7315\nEval Loss: 0.7553\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15\nTrain Loss: 0.7068\nEval Loss: 0.6619\n--------------------------------------\nFound the best model at Epoch 4\nValidation Loss reduced from 0.6959 to 0.6619\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15\nTrain Loss: 0.6963\nEval Loss: 0.9148\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:34<00:00,  3.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15\nTrain Loss: 0.6910\nEval Loss: 0.6495\n--------------------------------------\nFound the best model at Epoch 6\nValidation Loss reduced from 0.6619 to 0.6495\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:35<00:00,  3.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15\nTrain Loss: 0.6770\nEval Loss: 0.6505\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15\nTrain Loss: 0.6736\nEval Loss: 0.6544\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:34<00:00,  3.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15\nTrain Loss: 0.6703\nEval Loss: 0.8404\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:35<00:00,  3.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15\nTrain Loss: 0.6600\nEval Loss: 0.7220\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:36<00:00,  3.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15\nTrain Loss: 0.6619\nEval Loss: 0.6737\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15\nTrain Loss: 0.6600\nEval Loss: 0.6483\n--------------------------------------\nFound the best model at Epoch 12\nValidation Loss reduced from 0.6495 to 0.6483\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:36<00:00,  3.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15\nTrain Loss: 0.6584\nEval Loss: 0.7328\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15\nTrain Loss: 0.6569\nEval Loss: 0.6339\n--------------------------------------\nFound the best model at Epoch 14\nValidation Loss reduced from 0.6483 to 0.6339\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:32<00:00,  3.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15\nTrain Loss: 0.6590\nEval Loss: 0.6552\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Progressive Unfreezing**","metadata":{}},{"cell_type":"code","source":"# Visualize progress bar\nfrom tqdm import tqdm\nfrom transformers import AdamW\n\nif not isSubmit:\n    \n    # Prepare optimizer\n    LR_RATE = 3e-4\n    LR_DECAY = 0.1\n    optimizer = AdamW(model.parameters(), lr=LR_RATE)\n    NUM_EPOCHS = 25\n    EPOCH_STEPS = [11, 16, 21]\n    \n    best_eval_loss = float('inf')  # Initialize the best evaluation loss to infinity\n    \n    STEP = 2\n    for epoch in range(NUM_EPOCHS):\n        \n        # Progressive Unfreezing\n        if epoch == EPOCH_STEPS[2-STEP]:\n            print(f'-------------------Progressive Unfreezing-------------------------------------\\n')\n            print(f'Unfreezing Encoder Layers {4*STEP} to {4*STEP + 4}')\n            for param in model.module.deberta.encoder.layer[4*STEP:4*STEP + 4].parameters():\n                param.requires_grad = True\n\n            total_params = sum(p.numel() for p in model.module.parameters() if p.requires_grad)\n            print(\"\\nTotal Trainable parameters: \", total_params)\n            print(f'Lowering the Learning Rate...')\n            LR_RATE = LR_RATE *  LR_DECAY\n            print(f'Updating the Optimizer with new learning rate of {LR_RATE}')\n            optimizer = AdamW(model.parameters(), lr=LR_RATE)\n            STEP = STEP - 1\n            print(f'--------------------------------------------------------------------------------\\n')\n\n        total_loss = 0\n        model.train() # Set the model to Training mode\n\n        for batch in tqdm(train_dataloader):\n\n            with autocast():\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                # Forward loop\n                optimizer.zero_grad() # Ensures Gradient doesn't accumulate.\n                predictions = model(input_ids=input_ids,\n                                    attention_mask=attention_mask, \n                                    labels=labels\n                                   ).logits\n\n                # Compute MCRMSE loss\n                loss = mcrmse_loss(predictions, labels)\n\n            # BackProp\n            scaler.scale(loss).backward()  # Scale the loss value\n            scaler.step(optimizer)\n            scaler.update()\n\n            # Accumulate the Loss\n            total_loss += loss.item()\n\n        # Calculate epoch-level metrics\n        epoch_loss = total_loss / len(train_dataloader)\n\n        # Print epoch-level metrics\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        print(f\"Train Loss: {epoch_loss:.4f}\")\n\n         # Evaluation\n        model.eval()  # Set model to evaluation mode\n        eval_loss = 0\n\n        with torch.no_grad():\n            for batch in eval_dataloader:\n                with autocast():\n                    input_ids = batch['input_ids'].to(device)\n                    attention_mask = batch['attention_mask'].to(device)\n                    labels = batch['labels'].to(device)\n\n                    predictions = model(input_ids=input_ids,\n                                        attention_mask=attention_mask, \n                                        labels=labels\n                                       ).logits\n                    loss = mcrmse_loss(predictions, labels)\n\n                    eval_loss += loss.item()\n\n        # Calculate evaluation metrics\n        eval_epoch_loss = eval_loss / len(eval_dataloader)\n\n        # Print evaluation metrics\n        print(f\"Eval Loss: {eval_epoch_loss:.4f}\")\n\n        # Save the Best Model\n        if eval_epoch_loss < best_eval_loss:\n            print(f'--------------------------------------')\n            print(f'Found the best model at Epoch {epoch+1}')\n            print(f'Validation Loss reduced from {best_eval_loss:.4f} to {eval_epoch_loss:.4f}')\n            best_eval_loss = eval_epoch_loss\n            print(f'Saving the best model.')\n            print(f'--------------------------------------\\n')\n            \n            # Access the actual model from the DataParallel object\n            actual_model = model.module\n            # Save\n            actual_model.save_pretrained(\"debertaBaseV3_3b9047\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:54:50.577451Z","iopub.execute_input":"2023-08-27T13:54:50.577912Z","iopub.status.idle":"2023-08-27T14:41:21.697451Z","shell.execute_reply.started":"2023-08-27T13:54:50.577873Z","shell.execute_reply":"2023-08-27T14:41:21.69614Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stderr","text":"100%|██████████| 323/323 [01:34<00:00,  3.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\nTrain Loss: 0.8801\nEval Loss: 0.7506\n--------------------------------------\nFound the best model at Epoch 1\nValidation Loss reduced from inf to 0.7506\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/25\nTrain Loss: 0.7647\nEval Loss: 0.7065\n--------------------------------------\nFound the best model at Epoch 2\nValidation Loss reduced from 0.7506 to 0.7065\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/25\nTrain Loss: 0.7303\nEval Loss: 0.6863\n--------------------------------------\nFound the best model at Epoch 3\nValidation Loss reduced from 0.7065 to 0.6863\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/25\nTrain Loss: 0.7139\nEval Loss: 0.6941\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/25\nTrain Loss: 0.6870\nEval Loss: 0.6667\n--------------------------------------\nFound the best model at Epoch 5\nValidation Loss reduced from 0.6863 to 0.6667\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/25\nTrain Loss: 0.6777\nEval Loss: 0.6627\n--------------------------------------\nFound the best model at Epoch 6\nValidation Loss reduced from 0.6667 to 0.6627\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:32<00:00,  3.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/25\nTrain Loss: 0.6745\nEval Loss: 0.6689\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/25\nTrain Loss: 0.6761\nEval Loss: 0.6481\n--------------------------------------\nFound the best model at Epoch 8\nValidation Loss reduced from 0.6627 to 0.6481\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/25\nTrain Loss: 0.6729\nEval Loss: 0.6892\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/25\nTrain Loss: 0.6628\nEval Loss: 0.6644\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:33<00:00,  3.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/25\nTrain Loss: 0.6571\nEval Loss: 0.6482\n-------------------Progressive Unfreezing-------------------------------------\n\nUnfreezing Encoder Layers 8 to 12\n\nTotal Trainable parameters:  28943618\nLowering the Learning Rate...\nUpdating the Optimizer with new learning rate of 2.9999999999999997e-05\n--------------------------------------------------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:47<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/25\nTrain Loss: 0.5702\nEval Loss: 0.6376\n--------------------------------------\nFound the best model at Epoch 12\nValidation Loss reduced from 0.6481 to 0.6376\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:47<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/25\nTrain Loss: 0.4991\nEval Loss: 0.6525\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:46<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/25\nTrain Loss: 0.4703\nEval Loss: 0.8572\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:46<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/25\nTrain Loss: 0.4412\nEval Loss: 0.6144\n--------------------------------------\nFound the best model at Epoch 15\nValidation Loss reduced from 0.6376 to 0.6144\nSaving the best model.\n--------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [01:47<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/25\nTrain Loss: 0.4294\nEval Loss: 0.6471\n-------------------Progressive Unfreezing-------------------------------------\n\nUnfreezing Encoder Layers 4 to 8\n\nTotal Trainable parameters:  57295106\nLowering the Learning Rate...\nUpdating the Optimizer with new learning rate of 3e-06\n--------------------------------------------------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [02:01<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/25\nTrain Loss: 0.3698\nEval Loss: 0.6420\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [02:00<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/25\nTrain Loss: 0.3545\nEval Loss: 0.6237\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [02:01<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/25\nTrain Loss: 0.3471\nEval Loss: 0.6616\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [02:00<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/25\nTrain Loss: 0.3374\nEval Loss: 0.6436\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [02:00<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/25\nTrain Loss: 0.3302\nEval Loss: 0.6464\n-------------------Progressive Unfreezing-------------------------------------\n\nUnfreezing Encoder Layers 0 to 4\n\nTotal Trainable parameters:  85646594\nLowering the Learning Rate...\nUpdating the Optimizer with new learning rate of 3.0000000000000004e-07\n--------------------------------------------------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 323/323 [02:15<00:00,  2.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/25\nTrain Loss: 0.3204\nEval Loss: 0.6441\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m20\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m epoch \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(NUM_EPOCHS):                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 18 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 19 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Progressive Unfreezing\u001b[0m                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 20 \u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m epoch == EPOCH_STEPS[\u001b[94m2\u001b[0m-STEP]:                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33m-------------------Progressive Unfreezing---------------------------\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mUnfreezing Encoder Layers \u001b[0m\u001b[33m{\u001b[0m\u001b[94m4\u001b[0m*STEP\u001b[33m}\u001b[0m\u001b[33m to \u001b[0m\u001b[33m{\u001b[0m\u001b[94m4\u001b[0m*STEP\u001b[90m \u001b[0m+\u001b[90m \u001b[0m\u001b[94m4\u001b[0m\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 23 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m param \u001b[95min\u001b[0m model.module.deberta.encoder.layer[\u001b[94m4\u001b[0m*STEP:\u001b[94m4\u001b[0m*STEP + \u001b[94m4\u001b[0m].parameter   \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mIndexError: \u001b[0mlist index out of range\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> epoch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(NUM_EPOCHS):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Progressive Unfreezing</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 20 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> epoch == EPOCH_STEPS[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>-STEP]:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'-------------------Progressive Unfreezing---------------------------</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'Unfreezing Encoder Layers {</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>*STEP<span style=\"color: #808000; text-decoration-color: #808000\">} to {</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>*STEP<span style=\"color: #808080; text-decoration-color: #808080\"> </span>+<span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span><span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> param <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> model.module.deberta.encoder.layer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>*STEP:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>*STEP + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>].parameter   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>list index out of range\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Inference","metadata":{}},{"cell_type":"markdown","source":"Prepare Test Data Loader.","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 256\nTOKZ = \"/kaggle/input/commonlit-summaries-all-tokenizers/deberta_base_v3_tokz/deberta_base_v3_tokz\"\n\n# Read Test DF's.\nprompt_test_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\nsummary_test_df = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n\n# Initialize Dataset Classes\ncommonlit_summary_test_ds = CommonLitSummaryDataset(summary_test_df,\n                                                    prompt_test_df, \n                                                    tokz = TOKZ, \n                                                    max_length = MAX_LENGTH,\n                                                    isTest = True\n                                                    )\n# Test Dataloader\nbatch_size = 16\ntest_loader = DataLoader(commonlit_summary_test_ds, batch_size=batch_size, shuffle=False)\n\ntest_loader","metadata":{"execution":{"iopub.status.busy":"2023-08-27T10:59:54.01674Z","iopub.execute_input":"2023-08-27T10:59:54.017226Z","iopub.status.idle":"2023-08-27T10:59:54.270093Z","shell.execute_reply.started":"2023-08-27T10:59:54.017186Z","shell.execute_reply":"2023-08-27T10:59:54.268988Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7fccfe6a2b00>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Prediction**","metadata":{}},{"cell_type":"code","source":"# Variable to set True for 4-Fold Inference.\nisKFold = False","metadata":{"execution":{"iopub.status.busy":"2023-08-27T10:59:55.100588Z","iopub.execute_input":"2023-08-27T10:59:55.101342Z","iopub.status.idle":"2023-08-27T10:59:55.107094Z","shell.execute_reply.started":"2023-08-27T10:59:55.101298Z","shell.execute_reply":"2023-08-27T10:59:55.105888Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"**For a Single Fold Model Inference.**","metadata":{}},{"cell_type":"code","source":"# Eval model\nif not isKFold:\n    \n    # Load Model\n    model_path = '/kaggle/input/commonlit-summaries-all-models/deberta_base_v3/deberta_base_v3/39c16e'\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n    # Check if GPU is available.\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'DEVICE: {device}\\n')\n\n    # Model to the Device\n    model = model.to(device)\n    model = DataParallel(model) \n    model.eval()\n\n    predictions = []\n    with torch.no_grad():  # Disable gradient computation during inference\n        for batch in tqdm(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            # Predictions\n            predictions_batch = model(input_ids=input_ids,\n                                      attention_mask=attention_mask\n                                     ).logits\n\n            # Collect predictions from this batch\n            predictions.extend(predictions_batch.cpu().tolist())\n\n\n    # Convert to numpy\n    import numpy as np\n    predictions = np.array(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:00:03.095423Z","iopub.execute_input":"2023-08-27T11:00:03.095819Z","iopub.status.idle":"2023-08-27T11:00:03.104909Z","shell.execute_reply.started":"2023-08-27T11:00:03.095788Z","shell.execute_reply":"2023-08-27T11:00:03.103408Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"**For 4-Fold Model Inference**","metadata":{}},{"cell_type":"code","source":"if isKFold:\n    \n    import numpy as np\n    # Check if GPU is available.\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'DEVICE: {device}\\n')\n    \n    # Provide the List of all model paths.\n    model_paths = ['/kaggle/input/commonlit-summaries-all-models/deberta_base_v3/deberta_base_v3/39c16e', \n                   '/kaggle/input/commonlit-summaries-all-models/deberta_base_v3/deberta_base_v3/3b9047', \n                   '/kaggle/input/commonlit-summaries-all-models/deberta_base_v3/deberta_base_v3/814d6b', \n                   '/kaggle/input/commonlit-summaries-all-models/deberta_base_v3/deberta_base_v3/ebad26'\n                  ]\n    \n    # 4-Fold Predictions\n    fold_4_predictions = []\n    \n    # Predictions\n    for path in model_paths:\n        stratified_id = path.split('/')[-1]\n        print(f'Fold-ID: {stratified_id}')\n        model = AutoModelForSequenceClassification.from_pretrained(path)\n        model = model.to(device)\n        model = DataParallel(model) \n        model.eval()\n\n        predictions = []\n        with torch.no_grad():  # Disable gradient computation during inference\n            for batch in tqdm(test_loader):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n\n                # Predictions\n                predictions_batch = model(input_ids=input_ids,\n                                          attention_mask=attention_mask\n                                         ).logits\n\n                # Collect predictions from this batch\n                predictions.extend(predictions_batch.cpu().tolist())\n        \n        # Append\n        fold_4_predictions.append(predictions)\n        \n    # Predictions    \n    predictions = np.mean(np.array(fold_4_predictions), axis=0) ","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:00:09.075865Z","iopub.execute_input":"2023-08-27T11:00:09.077247Z","iopub.status.idle":"2023-08-27T11:00:27.173287Z","shell.execute_reply.started":"2023-08-27T11:00:09.077197Z","shell.execute_reply":"2023-08-27T11:00:27.171779Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"DEVICE: cuda\n\nFold-ID: 39c16e\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold-ID: 3b9047\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold-ID: 814d6b\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold-ID: ebad26\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Submission**","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame\ndata = {\n    'student_id': summary_test_df['student_id'].tolist(),\n    'content': predictions[:,0],\n    'wording': predictions[:,1]\n}\nsubmission_df = pd.DataFrame(data)\n\n# Display\ndisplay(submission_df.head())\n\n# Save it for Submission\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T11:00:30.843556Z","iopub.execute_input":"2023-08-27T11:00:30.843942Z","iopub.status.idle":"2023-08-27T11:00:30.860529Z","shell.execute_reply.started":"2023-08-27T11:00:30.843911Z","shell.execute_reply":"2023-08-27T11:00:30.859176Z"},"trusted":true},"execution_count":91,"outputs":[{"output_type":"display_data","data":{"text/plain":"     student_id   content   wording\n0  000000ffffff -1.544345 -1.197182\n1  111111eeeeee -1.544267 -1.198148\n2  222222cccccc -1.544800 -1.195079\n3  333333dddddd -1.542942 -1.194006","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>-1.544345</td>\n      <td>-1.197182</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111111eeeeee</td>\n      <td>-1.544267</td>\n      <td>-1.198148</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222222cccccc</td>\n      <td>-1.544800</td>\n      <td>-1.195079</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>-1.542942</td>\n      <td>-1.194006</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Performance Tracker\n\n**1. Bert-Base-Cased**\n\n<style>\ntable {\n    width: 100%;\n    border-collapse: collapse;\n}\n\nth, td {\n    padding: 8px;\n    text-align: left;\n}\n\nth {\n    background-color: #FF0000; /* Red color */\n    color: white;\n}\n</style>\n\n<table>\n    <tr>\n        <th><span style=\"color:red\">S.No.</span></th>\n        <th><span style=\"color:red\">Seed/ID</span></th>\n        <th><span style=\"color:red\">Split</span></th>\n        <th><span style=\"color:red\">Model_name</span></th>\n        <th><span style=\"color:red\">Input Tokens</span></th>\n        <th><span style=\"color:red\">CV</span></th>\n        <th><span style=\"color:red\">LB</span></th>\n    </tr>\n    <tr>\n        <td>1</td>\n        <td>3b9047</td>\n        <td>Stratify</td>\n        <td>bert-base-cased</td>\n        <td>256</td>\n        <td>0.6537</td>\n        <td>0.544</td>\n    </tr>\n    <tr>\n        <td>2</td>\n        <td>814d6b</td>\n        <td>Stratify</td>\n        <td>bert-base-cased</td>\n        <td>256</td>\n        <td>0.6895</td>\n        <td>0.566</td>\n    </tr>\n    <tr>\n        <td>3</td>\n        <td>ebad26</td>\n        <td>Stratify</td>\n        <td>bert-base-cased</td>\n        <td>256</td>\n        <td>0.5605</td>\n        <td>0.547</td>\n    </tr>\n    <tr>\n        <td>4</td>\n        <td>39c16e</td>\n        <td>Stratify</td>\n        <td>bert-base-cased</td>\n        <td>256</td>\n        <td>0.5567</td>\n        <td>0.603</td>\n    </tr>\n    <tr style=\"background-color: lightgreen;\">\n        <td>5</td>\n        <td>4-Fold</td>\n        <td>Stratify</td>\n        <td>bert-base-cased</td>\n        <td>256</td>\n        <td>NA</td>\n        <td>0.530</td>\n    </tr>\n \n    \n</table>\n\n\n\n**2. Deberta-base-v3**\n\n<style>\ntable {\n    width: 100%;\n    border-collapse: collapse;\n}\n\nth, td {\n    padding: 8px;\n    text-align: left;\n}\n\nth {\n    background-color: #FF0000; /* Red color */\n    color: white;\n}\n</style>\n\n<table>\n    <tr>\n        <th><span style=\"color:red\">S.No.</span></th>\n        <th><span style=\"color:red\">Seed/ID</span></th>\n        <th><span style=\"color:red\">Split</span></th>\n        <th><span style=\"color:red\">Model_name</span></th>\n        <th><span style=\"color:red\">Input Tokens</span></th>\n        <th><span style=\"color:red\">CV</span></th>\n        <th><span style=\"color:red\">LB</span></th>\n    </tr>\n    <tr>\n        <td>1</td>\n        <td>3b9047</td>\n        <td>Stratify</td>\n        <td>deberta-base-v3</td>\n        <td>256</td>\n        <td>0.6020</td>\n        <td>0.516</td>\n    </tr>\n    <tr>\n        <td>2</td>\n        <td>814d6b</td>\n        <td>Stratify</td>\n        <td>deberta-base-v3</td>\n        <td>256</td>\n        <td>0.5697</td>\n        <td>0.526</td>\n    </tr>\n    <tr>\n        <td>3</td>\n        <td>ebad26</td>\n        <td>Stratify</td>\n        <td>deberta-base-v3</td>\n        <td>256</td>\n        <td>0.4932</td>\n        <td>0.525</td>\n    </tr>\n    <tr>\n        <td>4</td>\n        <td>39c16e</td>\n        <td>Stratify</td>\n        <td>deberta-base-v3</td>\n        <td>256</td>\n        <td>0.4729</td>\n        <td>0.502</td>\n    </tr>\n        <tr style=\"background-color: lightgreen;\">\n        <td>5</td>\n        <td>4-Fold</td>\n        <td>Stratify</td>\n        <td>deberta-base-v3</td>\n        <td>256</td>\n        <td>NA</td>\n        <td>0.495</td>\n    </tr>\n    \n</table>\n\n","metadata":{}},{"cell_type":"markdown","source":"**TODO:**\n\n* FP16 Training <font color=\"green\">&#10004;</font>\n* Utilize Both of the GPU's <font color=\"green\">&#10004;</font>\n* Stratified Split <font color=\"green\">&#10004;</font>\n* Data Augmentation - CutMix, Back Translation <font color=\"green\">&#10004;</font> (--**Did NOT worked**--)\n* CommonLit MLM Training --> Downstream Task  <font color=\"green\">&#10004;</font> (--**Did NOT worked**--)\n* Model Architecture Tweaking <font color=\"green\">&#10004;</font>\n* Freeze Top-n Layers\n* Learning Rate Schedular\n* Pseudo Labeling\n* Experiment with the Loss Function.\n* Analyse content and wording score separately.\n* Evaluation on Steps rather than Epochs.\n* Try Deberta-v3-base + SiFT\n* Experiment with the MAX_LENGTH. (Try 512)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}